{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0313355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef21cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"../../Data/Testdata_Models/AirQualityUCI.xlsx\")\n",
    "\n",
    "data[\"datetime\"] = pd.to_datetime(data[\"Date\"].astype(str) + \" \" + data[\"Time\"].astype(str))\n",
    "data.drop(columns=['PT08.S1(CO)', 'NMHC(GT)', 'C6H6(GT)', 'PT08.S2(NMHC)', 'NOx(GT)', 'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)'], inplace=True)\n",
    "\n",
    "data = data.sort_values(\"datetime\").reset_index(drop=True)\n",
    "data = data[[\"datetime\", \"CO(GT)\", \"T\", \"RH\", \"AH\"]].rename(columns={\n",
    "    \"CO(GT)\": \"Y\",\n",
    "})\n",
    "\n",
    "data = data.replace(-200, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc940cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 2004-03-10 18:00:00 Max: 2005-04-04 14:00:00\n"
     ]
    }
   ],
   "source": [
    "# range of values\n",
    "df = data.copy()\n",
    "print(f\"Min: {df['datetime'].min()} Max: {df['datetime'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a27e594e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before dropping Nans: (9357, 5)\n",
      "Shape after dropping Nans: (7344, 5)\n"
     ]
    }
   ],
   "source": [
    "# How many varibales are nan\n",
    "print(f\"Shape before dropping Nans: {df.shape}\")\n",
    "df = df.dropna()\n",
    "print(f\"Shape after dropping Nans: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cb92048",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(\"datetime\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77a76fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/4117726918.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{target}_lag{k}\"] = out[target].shift(k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3252, 144)\n"
     ]
    }
   ],
   "source": [
    "# --- build AR lags and drop NaNs (keep only Y_lag* as features) ---\n",
    "def make_ar_lags(df, target=\"Y\", lags=72):\n",
    "    out = df[[target]].copy()\n",
    "    for k in range(1, lags+1):\n",
    "        out[f\"{target}_lag{k}\"] = out[target].shift(k)\n",
    "    return out.dropna()\n",
    "\n",
    "lags = 144  # use as much history as you want (e.g., 3 days if hourly)\n",
    "ar = make_ar_lags(df, \"Y\", lags=lags)\n",
    "\n",
    "# split\n",
    "train = ar.loc[ar.index < \"2004-11-01\"]\n",
    "test  = ar.loc[ar.index >= \"2004-11-01\"]\n",
    "\n",
    "lag_cols = [c for c in train.columns if c.startswith(\"Y_lag\")]\n",
    "X_train, y_train = train[lag_cols], train[\"Y\"]\n",
    "X_test,  y_test  = test[lag_cols],  test[\"Y\"]\n",
    "\n",
    "# model\n",
    "model = LinearRegression().fit(X_train, y_train)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54f76317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- recursive k-step forecast helper (pure AR) ---\n",
    "# def recursive_ar_forecast(model, lag_vector, lag_cols, steps):\n",
    "#     # coerce to 1D numpy array\n",
    "#     if isinstance(lag_vector, pd.Series):\n",
    "#         lag_vals = lag_vector.values\n",
    "#     else:\n",
    "#         lag_vals = np.asarray(lag_vector)\n",
    "\n",
    "#     preds = []\n",
    "#     for _ in range(steps):\n",
    "#         X = pd.DataFrame([lag_vals], columns=lag_cols)  # keep feature names\n",
    "#         y_hat = model.predict(X)[0]\n",
    "#         preds.append(y_hat)\n",
    "#         # shift: insert prediction at front, drop last lag\n",
    "#         lag_vals = np.concatenate(([y_hat], lag_vals[:-1]))\n",
    "#     return np.array(preds)\n",
    "\n",
    "\n",
    "# block_len = 5  # number of rows per forecast block (set to 3 if you want 3-step horizon)\n",
    "\n",
    "# predictions, truths, timestamps = [], [], []\n",
    "# lag_cols = list(X_train.columns)\n",
    "\n",
    "# i = 0\n",
    "# test_idx = X_test.index\n",
    "# while i < len(test_idx):\n",
    "#     print(i)\n",
    "#     idx_block = test_idx[i : i + block_len]            # exactly last 5 rows of this chunk\n",
    "#     if len(idx_block) == 0:\n",
    "#         break\n",
    "\n",
    "#     # initial lags for the first timestamp in this block\n",
    "#     lag_vec = X_test.loc[idx_block[0], lag_cols]       # Series with names\n",
    "\n",
    "#     # recursive 5-step (len(idx_block)) forecast\n",
    "#     preds_block = recursive_ar_forecast(\n",
    "#         model=model,\n",
    "#         lag_vector=lag_vec,\n",
    "#         lag_cols=lag_cols,\n",
    "#         steps=len(idx_block)\n",
    "#     )\n",
    "\n",
    "#     predictions.extend(preds_block)\n",
    "#     truths.extend(y_test.loc[idx_block].to_numpy())\n",
    "#     timestamps.extend(idx_block.tolist())\n",
    "\n",
    "#     i += block_len    \n",
    "# results = pd.DataFrame({\"datetime\": timestamps, \"y_true\": truths, \"y_pred\": predictions}).set_index(\"datetime\")\n",
    "# rmse = mean_squared_error(results[\"y_true\"], results[\"y_pred\"])  # RMSE\n",
    "# print(f\"RMSE (5-step recursive AR): {rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f8941",
   "metadata": {},
   "source": [
    "### With covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3500345",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # --- 1) Build lag features for Y and covariates ---\n",
    "# def make_lagged_features(df, target=\"Y\", covariates=(\"T\",\"RH\",\"AH\"), lags=72):\n",
    "#     # Build a single list of Series/DataFrames, then concat once (no repeated inserts)\n",
    "#     parts = [df[[target]]]  # keep original target (for y)\n",
    "#     # target lags\n",
    "#     parts += [df[target].shift(k).rename(f\"{target}_lag{k}\") for k in range(1, lags + 1)]\n",
    "#     # covariate lags\n",
    "#     for c in covariates:\n",
    "#         parts += [df[c].shift(k).rename(f\"{c}_lag{k}\") for k in range(1, lags + 1)]\n",
    "#     out = pd.concat(parts, axis=1)\n",
    "#     return out.dropna()\n",
    "\n",
    "# lags = 72           # history length (keep as you like)\n",
    "# block_len = 5       # <- your “last 5 entries” per block (set to 3 if you want 3-step)\n",
    "\n",
    "# lagged = make_lagged_features(df, target=\"Y\", covariates=(\"T\",\"RH\",\"AH\"), lags=lags)\n",
    "\n",
    "# # --- 2) Train/test split (November start) ---\n",
    "# train = lagged.loc[lagged.index < \"2004-11-01\"]\n",
    "# test  = lagged.loc[lagged.index >= \"2004-11-01\"]\n",
    "\n",
    "# y_col = \"Y\"\n",
    "# y_lag_cols  = [f\"Y_lag{k}\" for k in range(1, lags+1)]\n",
    "# cov_lag_cols = [c for c in lagged.columns if c.startswith(\"T_lag\") or c.startswith(\"RH_lag\") or c.startswith(\"AH_lag\")]\n",
    "# feature_cols = y_lag_cols + cov_lag_cols  # keep a fixed, known order\n",
    "\n",
    "# X_train, y_train = train[feature_cols], train[y_col]\n",
    "# X_test,  y_test  = test[feature_cols],  test[y_col]\n",
    "\n",
    "# # --- 3) Fit linear baseline ---\n",
    "# model = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# # --- 4) Recursive multi-step forecast in fixed-size row blocks (no time windowing) ---\n",
    "# predictions, truths, timestamps = [], [], []\n",
    "# test_idx = X_test.index\n",
    "# i = 0\n",
    "\n",
    "# while i < len(test_idx):\n",
    "#     idx_block = test_idx[i : i + block_len]\n",
    "#     if len(idx_block) == 0:\n",
    "#         break\n",
    "\n",
    "#     # initialize Y lags from the first row of the block\n",
    "#     y_lags = X_test.loc[idx_block[0], y_lag_cols].to_numpy()\n",
    "\n",
    "#     # step through the block recursively\n",
    "#     for ts in idx_block:\n",
    "#         row = X_test.loc[ts, feature_cols].copy()  # covariate lags for this timestamp\n",
    "#         # overwrite Y lag columns with our *current* y_lags (so we don't use future truth)\n",
    "#         row.loc[y_lag_cols] = y_lags\n",
    "#         y_hat = model.predict(pd.DataFrame([row], columns=feature_cols))[0]\n",
    "\n",
    "#         predictions.append(y_hat)\n",
    "#         truths.append(y_test.loc[ts])\n",
    "#         timestamps.append(ts)\n",
    "\n",
    "#         # shift in our prediction for the next step\n",
    "#         y_lags = np.concatenate(([y_hat], y_lags[:-1]))\n",
    "\n",
    "#     i += block_len\n",
    "\n",
    "# results = pd.DataFrame(\n",
    "#     {\"datetime\": timestamps, \"y_true\": truths, \"y_pred\": predictions}\n",
    "# ).set_index(\"datetime\")\n",
    "\n",
    "# rmse = mean_squared_error(results[\"y_true\"], results[\"y_pred\"])\n",
    "# print(f\"RMSE (recursive with Y + covariate lags): {rmse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "991e6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tscv = TimeSeriesSplit(n_splits=5)\n",
    "# model = Pipeline([\n",
    "#     (\"scaler\", StandardScaler()),\n",
    "#     (\"reg\", RidgeCV(alphas=np.logspace(-4, 3, 60), cv=tscv))\n",
    "# ])\n",
    "# model.fit(X_train, y_train)\n",
    "# predictions, truths, timestamps = [], [], []\n",
    "# test_idx = X_test.index\n",
    "# i = 0\n",
    "\n",
    "\n",
    "# while i < len(test_idx):\n",
    "#     idx_block = test_idx[i : i + block_len]\n",
    "#     if len(idx_block) == 0:\n",
    "#         break\n",
    "\n",
    "#     # initialize Y lags from the first row of the block\n",
    "#     y_lags = X_test.loc[idx_block[0], y_lag_cols].to_numpy()\n",
    "#     print(i)\n",
    "#     # step through the block recursively\n",
    "#     for ts in idx_block:\n",
    "#         row = X_test.loc[ts, feature_cols].copy()  # covariate lags for this timestamp\n",
    "#         # overwrite Y lag columns with our *current* y_lags (so we don't use future truth)\n",
    "#         row.loc[y_lag_cols] = y_lags\n",
    "#         y_hat = model.predict(pd.DataFrame([row], columns=feature_cols))[0]\n",
    "\n",
    "#         predictions.append(y_hat)\n",
    "#         truths.append(y_test.loc[ts])\n",
    "#         timestamps.append(ts)\n",
    "\n",
    "#         # shift in our prediction for the next step\n",
    "#         y_lags = np.concatenate(([y_hat], y_lags[:-1]))\n",
    "\n",
    "#     i += block_len\n",
    "\n",
    "# results = pd.DataFrame(\n",
    "#     {\"datetime\": timestamps, \"y_true\": truths, \"y_pred\": predictions}\n",
    "# ).set_index(\"datetime\")\n",
    "\n",
    "# rmse = mean_squared_error(results[\"y_true\"], results[\"y_pred\"])\n",
    "# print(f\"RMSE (recursive with Y + covariate lags): {rmse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99dc29bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- TimesFM: batched block forecasts on your series Y ---\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# import timesfm\n",
    "\n",
    "# # 0) Model (GPU if available)\n",
    "# torch.set_float32_matmul_precision(\"high\")\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = \"mps\"  # Metal Performance Shaders (for Apple Silicon)\n",
    "# elif torch.cuda.is_available():\n",
    "#     device = \"cuda\"\n",
    "# else:\n",
    "#     device = \"cpu\"\n",
    "\n",
    "# model = timesfm.TimesFM_2p5_200M_torch.from_pretrained(\"google/timesfm-2.5-200m-pytorch\")\n",
    "# cfg = timesfm.ForecastConfig(\n",
    "#     max_context=1024,         # keep last 1024 points of history per block\n",
    "#     max_horizon=256,\n",
    "#     normalize_inputs=True,    # TimesFM handles normalization internally\n",
    "#     use_continuous_quantile_head=True,\n",
    "#     force_flip_invariance=True,\n",
    "#     infer_is_positive=False,  # set True only if your Y is strictly >= 0\n",
    "#     fix_quantile_crossing=True,\n",
    "# )\n",
    "# model.compile(cfg)\n",
    "\n",
    "# # 1) Your series: df indexed by datetime, column 'Y'\n",
    "# # df = df.set_index(\"datetime\").sort_index()\n",
    "# y = df[\"Y\"].astype(\"float32\")\n",
    "\n",
    "# # 2) Split and block settings\n",
    "# split_ts  = pd.Timestamp(\"2004-11-01\")  # start of test\n",
    "# block_len = 5                           # use 3 if you want 3-step horizon\n",
    "\n",
    "# test_y  = y.loc[y.index >= split_ts]\n",
    "# test_idx = test_y.index\n",
    "\n",
    "# # Build block start *row* indices over the test range (last-N-rows style)\n",
    "# starts = np.arange(0, len(test_idx), block_len)\n",
    "# block_starts = [test_idx[s] for s in starts]\n",
    "\n",
    "# # 3) Build per-block contexts = all history strictly before each block start,\n",
    "# #    truncated to the last cfg.max_context points\n",
    "# def build_context(series: pd.Series, block_start_ts, max_context: int):\n",
    "#     # everything up to (but excluding) the block start timestamp\n",
    "#     end_loc = series.index.get_loc(block_start_ts)\n",
    "#     ctx = series.iloc[:end_loc].to_numpy(dtype=\"float32\")\n",
    "#     if ctx.size == 0:\n",
    "#         raise ValueError(\"Empty context before the first test timestamp.\")\n",
    "#     if ctx.size > max_context:\n",
    "#         ctx = ctx[-max_context:]\n",
    "#     return ctx\n",
    "\n",
    "# contexts = [build_context(y, t0, cfg.max_context) for t0 in block_starts]\n",
    "\n",
    "# print(\"4) Forecast all blocks at once (direct horizon = block_len)\")\n",
    "# with torch.inference_mode():\n",
    "#     point_fcst, quantile_fcst = model.forecast(horizon=block_len, inputs=contexts)\n",
    "#     # point_fcst.shape == (n_blocks, block_len)\n",
    "#     # quantile_fcst.shape == (n_blocks, block_len, 10)  # mean, then q10..q90\n",
    "\n",
    "# print(\"5) Stitch predictions back to timestamps and evaluate\")\n",
    "# preds, truths, times = [], [], []\n",
    "# for b, t0 in enumerate(block_starts):\n",
    "#     print(b,t0)\n",
    "#     idx_block = test_idx[starts[b] : starts[b] + block_len]      # these are the next block_len rows\n",
    "#     gt = test_y.loc[idx_block].to_numpy(dtype=\"float32\")\n",
    "#     ph = len(gt)                                                # last block may be shorter\n",
    "#     preds.extend(point_fcst[b, :ph].tolist())\n",
    "#     truths.extend(gt.tolist())\n",
    "#     times.extend(idx_block[:ph].tolist())\n",
    "\n",
    "# results = pd.DataFrame({\"datetime\": times, \"y_true\": truths, \"y_pred\": preds}).set_index(\"datetime\")\n",
    "# rmse = mean_squared_error(results[\"y_true\"], results[\"y_pred\"])\n",
    "# print(f\"RMSE (recursive with Y + covariate lags): {rmse:.3f}\")\n",
    "# rmse = mean_squared_error(results[\"y_true\"], results[\"y_pred\"])\n",
    "# print(f\"Device: {device}\")\n",
    "# print(f\"RMSE (TimesFM, {block_len}-step direct): {rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c714565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmse = mean_squared_error(results[\"y_true\"], results[\"y_pred\"])\n",
    "# print(f\"Device: {device}\")\n",
    "# print(f\"RMSE (TimesFM, {block_len}-step direct): {rmse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df18b7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages/lightning/fabric/__init__.py:41: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/2448691764.py:12: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  .asfreq(freq)                 # put missing hours in as NaN\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/2448691764.py:25: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  train_ar, test_template_ar = split(ds_ar, date=pd.Period(split_ts, freq=freq))\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "from gluonts.dataset.split import split\n",
    "from uni2ts.model.moirai2 import Moirai2Forecast, Moirai2Module\n",
    "\n",
    "# 0) Make the index uniform (hourly) and fill gaps\n",
    "freq = 'H'  # hourly\n",
    "dfu = (\n",
    "    df[['Y']]                       # AR-only\n",
    "      .sort_index()\n",
    "      .asfreq(freq)                 # put missing hours in as NaN\n",
    ")\n",
    "dfu['Y'] = dfu['Y'].ffill().bfill() # no peeking: forward-fill, then back-fill only if needed\n",
    "\n",
    "# 1) Split & windowing (same as TabPFN/TimesFM)\n",
    "split_ts  = pd.Timestamp(\"2004-11-01 00:00:00\")\n",
    "block_len = 5                       # set 3 if you want 3-step horizon\n",
    "TEST      = len(dfu.loc[dfu.index >= split_ts])\n",
    "N_WINDOWS = TEST // block_len       # non-overlapping blocks\n",
    "\n",
    "# 2) GluonTS dataset (target only)\n",
    "ds_ar = PandasDataset({\"series_0\": dfu['Y']}, freq=freq)\n",
    "\n",
    "train_ar, test_template_ar = split(ds_ar, date=pd.Period(split_ts, freq=freq))\n",
    "test_data_ar = test_template_ar.generate_instances(\n",
    "    prediction_length=block_len,\n",
    "    windows=N_WINDOWS,\n",
    "    distance=block_len,             # 0,5,10,… row-based blocks\n",
    ")\n",
    "\n",
    "# # 3) Moirai-2 small\n",
    "# CTX, BSZ = 1000, 32\n",
    "# model_ar = Moirai2Forecast(\n",
    "#     module=Moirai2Module.from_pretrained(\"Salesforce/moirai-2.0-R-small\"),\n",
    "#     prediction_length=block_len,\n",
    "#     context_length=CTX,\n",
    "#     target_dim=1,\n",
    "#     feat_dynamic_real_dim=0,\n",
    "#     past_feat_dynamic_real_dim=0,\n",
    "# )\n",
    "# predictor_ar = model_ar.create_predictor(batch_size=BSZ)\n",
    "\n",
    "# # 4) Forecast & evaluate\n",
    "# forecasts_ar = list(predictor_ar.predict(test_data_ar.input))\n",
    "# labels_ar    = list(test_data_ar.label)\n",
    "\n",
    "# y_pred_ar = np.concatenate([f.mean for f in forecasts_ar])\n",
    "# y_true_ar = np.concatenate([np.asarray(l[\"target\"]) for l in labels_ar])\n",
    "\n",
    "# rmse_ar = mean_squared_error(y_true_ar, y_pred_ar)\n",
    "# print(f\"RMSE (Moirai2 AR-only, {block_len}-step blocks): {rmse_ar:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7a6671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64543/3009199395.py:8: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df_canon = df[[\"Y\",\"T\",\"RH\",\"AH\"]].sort_index().asfreq(freq)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from uni2ts.model.moirai2 import Moirai2Forecast, Moirai2Module\n",
    "\n",
    "# 1) Canonical hourly data so all models compare apples-to-apples\n",
    "freq = \"H\"\n",
    "df_canon = df[[\"Y\",\"T\",\"RH\",\"AH\"]].sort_index().asfreq(freq)\n",
    "df_canon[\"Y\"] = df_canon[\"Y\"].ffill()  # no look-ahead for target\n",
    "for c in [\"T\",\"RH\",\"AH\"]:\n",
    "    df_canon[c] = df_canon[c].interpolate(method=\"time\").ffill().bfill()\n",
    "\n",
    "# 2) Split + blocks (exactly like your other models)\n",
    "split_ts  = pd.Timestamp(\"2004-11-01 00:00:00\")\n",
    "block_len = 5  # set 3 if you want 3-step horizon\n",
    "\n",
    "test_idx = df_canon.index[df_canon.index >= split_ts]\n",
    "starts   = np.arange(0, len(test_idx), block_len)\n",
    "\n",
    "# Helper: build a single-entry GluonTS dataset for a block's context\n",
    "def make_entry(y_series, end_ts, ctx, freq, t=None, rh=None, ah=None):\n",
    "    \"\"\"\n",
    "    end_ts: the first timestamp of the block (we forecast from just before this)\n",
    "    ctx:    how many last points to keep as context\n",
    "    \"\"\"\n",
    "    pos = y_series.index.get_loc(end_ts)\n",
    "    if isinstance(pos, slice):  # guard against duplicate indices\n",
    "        pos = pos.start\n",
    "    # history strictly before the block start\n",
    "    if pos == 0:\n",
    "        return None\n",
    "    y_hist = y_series.values[:pos].astype(\"float32\")\n",
    "    idx_hist = y_series.index[:pos]\n",
    "    # clip to last ctx points\n",
    "    if len(y_hist) > ctx:\n",
    "        y_hist  = y_hist[-ctx:]\n",
    "        idx_hist = idx_hist[-ctx:]\n",
    "    entry = {\n",
    "        \"start\": idx_hist[0],\n",
    "        \"target\": y_hist,\n",
    "    }\n",
    "    # add past covariates if provided (shape: F x time)\n",
    "    if t is not None and rh is not None and ah is not None:\n",
    "        T  = t.values[:pos].astype(\"float32\")\n",
    "        RH = rh.values[:pos].astype(\"float32\")\n",
    "        AH = ah.values[:pos].astype(\"float32\")\n",
    "        if len(T) > ctx:\n",
    "            T, RH, AH = T[-ctx:], RH[-ctx:], AH[-ctx:]\n",
    "        entry[\"past_feat_dynamic_real\"] = [T, RH, AH]\n",
    "    return ListDataset([entry], freq=freq)\n",
    "\n",
    "# Keep memory modest first; raise later if stable\n",
    "CTX, BSZ = 500, 4\n",
    "\n",
    "model_ar = Moirai2Forecast(\n",
    "    module=Moirai2Module.from_pretrained(\"Salesforce/moirai-2.0-R-small\"),\n",
    "    prediction_length=block_len,\n",
    "    context_length=CTX,\n",
    "    target_dim=1,\n",
    "    feat_dynamic_real_dim=0,\n",
    "    past_feat_dynamic_real_dim=0,\n",
    ")\n",
    "predictor_ar = model_ar.create_predictor(batch_size=BSZ)\n",
    "try: predictor_ar = predictor_ar.to(\"cpu\")  # avoids MPS/CUDA quirks\n",
    "except: pass\n",
    "\n",
    "sse, count, produced = 0.0, 0, 0\n",
    "for b in range(len(starts)):\n",
    "    start_ts = test_idx[starts[b]]\n",
    "    ds_one   = make_entry(df_canon[\"Y\"], end_ts=start_ts, ctx=CTX, freq=freq)\n",
    "    if ds_one is None:\n",
    "        continue\n",
    "\n",
    "    # One forecast for this block\n",
    "    f = next(predictor_ar.predict(ds_one))      # generator with 1 item\n",
    "    y_hat  = f.quantile(0.5).astype(\"float32\")  # explicit median, shape (block_len,)\n",
    "\n",
    "    # Ground truth for the block\n",
    "    idx_block = test_idx[starts[b] : starts[b] + block_len]\n",
    "    y_true = df_canon[\"Y\"].loc[idx_block].to_numpy(dtype=\"float32\")\n",
    "\n",
    "    ph = min(len(y_hat), len(y_true))           # tail-safe\n",
    "    diff = y_hat[:ph] - y_true[:ph]\n",
    "    sse  += float(np.dot(diff, diff))\n",
    "    count += ph\n",
    "    produced += 1\n",
    "\n",
    "mse_ar  = sse / count\n",
    "rmse_ar = (sse / count) ** 0.5\n",
    "print(f\"MSE  (Moirai2 AR-only, {block_len}-step blocks): {mse_ar:.3f}\")\n",
    "print(f\"RMSE (Moirai2 AR-only, {block_len}-step blocks): {rmse_ar:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f7ca42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE  (Moirai2 + past covariates, 5-step blocks): 1.333\n",
      "RMSE (Moirai2 + past covariates, 5-step blocks): 1.154\n"
     ]
    }
   ],
   "source": [
    "# --- Moirai-2 with past covariates (T, RH, AH) ---\n",
    "CTX, BSZ = 500, 4  # keep same as AR for fair comparison\n",
    "\n",
    "model_cov = Moirai2Forecast(\n",
    "    module=Moirai2Module.from_pretrained(\"Salesforce/moirai-2.0-R-small\"),\n",
    "    prediction_length=block_len,\n",
    "    context_length=CTX,\n",
    "    target_dim=1,\n",
    "    feat_dynamic_real_dim=0,      # no future-known features\n",
    "    past_feat_dynamic_real_dim=3, # T, RH, AH\n",
    ")\n",
    "predictor_cov = model_cov.create_predictor(batch_size=BSZ)\n",
    "try: predictor_cov = predictor_cov.to(\"cpu\")\n",
    "except: pass\n",
    "\n",
    "sse, count, produced = 0.0, 0, 0\n",
    "for b in range(len(starts)):\n",
    "    start_ts = test_idx[starts[b]]\n",
    "    ds_one = make_entry(\n",
    "        df_canon[\"Y\"], end_ts=start_ts, ctx=CTX, freq=freq,\n",
    "        t=df_canon[\"T\"], rh=df_canon[\"RH\"], ah=df_canon[\"AH\"]  # <-- pass covariates here\n",
    "    )\n",
    "    if ds_one is None:\n",
    "        continue\n",
    "\n",
    "    f = next(predictor_cov.predict(ds_one))\n",
    "    y_hat = f.quantile(0.5).astype(\"float32\")  # median as point forecast\n",
    "\n",
    "    idx_block = test_idx[starts[b] : starts[b] + block_len]\n",
    "    y_true = df_canon[\"Y\"].loc[idx_block].to_numpy(dtype=\"float32\")\n",
    "\n",
    "    ph = min(len(y_hat), len(y_true))\n",
    "    diff = y_hat[:ph] - y_true[:ph]\n",
    "    sse += float(np.dot(diff, diff))\n",
    "    count += ph\n",
    "    produced += 1\n",
    "\n",
    "mse_cov  = sse / count\n",
    "rmse_cov = (sse / count) ** 0.5\n",
    "print(f\"MSE  (Moirai2 + past covariates, {block_len}-step blocks): {mse_cov:.3f}\")\n",
    "print(f\"RMSE (Moirai2 + past covariates, {block_len}-step blocks): {rmse_cov:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564f40a2",
   "metadata": {},
   "source": [
    "## Flowstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b8365d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# ---------- 0) Canonical hourly data (so all models compare apples-to-apples) ----------\u001b[39;00m\n\u001b[32m      8\u001b[39m freq = \u001b[33m\"\u001b[39m\u001b[33mH\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m df_canon = \u001b[43mdf\u001b[49m[[\u001b[33m\"\u001b[39m\u001b[33mY\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mT\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mRH\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mAH\u001b[39m\u001b[33m\"\u001b[39m]].sort_index().asfreq(freq)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Causal fills (no look-ahead)\u001b[39;00m\n\u001b[32m     12\u001b[39m df_canon[\u001b[33m\"\u001b[39m\u001b[33mY\u001b[39m\u001b[33m\"\u001b[39m] = df_canon[\u001b[33m\"\u001b[39m\u001b[33mY\u001b[39m\u001b[33m\"\u001b[39m].ffill()\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tsfm_public import FlowStateForPrediction\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ---------- 0) Canonical hourly data (so all models compare apples-to-apples) ----------\n",
    "freq = \"H\"\n",
    "df_canon = df[[\"Y\",\"T\",\"RH\",\"AH\"]].sort_index().asfreq(freq)\n",
    "\n",
    "# Causal fills (no look-ahead)\n",
    "df_canon[\"Y\"] = df_canon[\"Y\"].ffill()\n",
    "for c in [\"T\",\"RH\",\"AH\"]:\n",
    "    df_canon[c] = df_canon[c].ffill()\n",
    "\n",
    "# ---------- 1) Shared split & blocks ----------\n",
    "split_ts  = pd.Timestamp(\"2004-11-01 00:00:00\")\n",
    "block_len = 5   # set 3 if that's your official horizon\n",
    "\n",
    "test_idx = df_canon.index[df_canon.index >= split_ts]\n",
    "starts   = np.arange(0, len(test_idx), block_len)  # 0,5,10,...\n",
    "\n",
    "# ---------- 2) Load FlowState predictor ----------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "predictor = FlowStateForPrediction.from_pretrained(\"ibm-research/flowstate\").to(device)\n",
    "\n",
    "# Keep this equal to your TabPFN lag count for fairness.\n",
    "CTX_LEN = 72\n",
    "BATCH   = 32\n",
    "SCALE   = 0.25\n",
    "Q_MED   = 0.5  # median\n",
    "\n",
    "# ---------- 3) Helper to build batched contexts (AR-only: 1 channel) ----------\n",
    "def build_batch_contexts_y(y: pd.Series, start_ts_list, ctx_len, device):\n",
    "    \"\"\"\n",
    "    Returns (ctx, kept_starts):\n",
    "      ctx: FloatTensor [ctx_len, batch, 1] (batch_first=False)\n",
    "      kept_starts: the subset of start_ts that had enough history\n",
    "    \"\"\"\n",
    "    yv  = y.to_numpy(dtype=\"float32\")\n",
    "    idx = y.index\n",
    "    batches = []\n",
    "    kept = []\n",
    "    for t0 in start_ts_list:\n",
    "        pos = idx.get_loc(t0)\n",
    "        if isinstance(pos, slice):\n",
    "            pos = pos.start\n",
    "        if pos < ctx_len:\n",
    "            continue\n",
    "        window = yv[pos - ctx_len : pos]          # strictly before t0\n",
    "        batches.append(window[:, None])           # (ctx_len, 1)\n",
    "        kept.append(t0)\n",
    "    if not batches:\n",
    "        return None, []\n",
    "    X = np.stack(batches, axis=1)                 # (ctx_len, batch, 1)\n",
    "    return torch.from_numpy(X).to(device), kept\n",
    "\n",
    "# ---------- 4a) DIRECT multi-horizon (faster; FlowState predicts block_len in one call) ----------\n",
    "def flowstate_direct(y: pd.Series, test_idx, starts, ctx_len, block_len, batch, device):\n",
    "    preds, trues, times = [], [], []\n",
    "    for i in range(0, len(starts), batch):\n",
    "        start_slice = [test_idx[s] for s in starts[i:i+batch] if s < len(test_idx)]\n",
    "        if not start_slice:\n",
    "            continue\n",
    "        ctx, kept = build_batch_contexts_y(y, start_slice, ctx_len, device)\n",
    "        if ctx is None:\n",
    "            continue\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            out = predictor(\n",
    "                ctx, scale_factor=SCALE, prediction_length=block_len, batch_first=False\n",
    "            )\n",
    "        po = out.prediction_outputs  # (batch, num_quantiles, horizon, 1)\n",
    "\n",
    "        # choose the closest available quantile to 0.5 (median)\n",
    "        if hasattr(out, \"quantile_values\"):\n",
    "            qs = torch.tensor(out.quantile_values, device=po.device)\n",
    "            q_idx = int(torch.argmin(torch.abs(qs - Q_MED)).item())\n",
    "        else:\n",
    "            q_idx = po.shape[1] // 2\n",
    "\n",
    "        y_hat = po[:, q_idx, :block_len, 0].detach().cpu().numpy()\n",
    "\n",
    "        # stitch per block\n",
    "        for j, t0 in enumerate(kept):\n",
    "            start_row = int(np.where(test_idx == t0)[0][0])\n",
    "            idx_block = test_idx[start_row : start_row + block_len]\n",
    "            y_true = y.loc[idx_block].to_numpy(dtype=\"float32\")\n",
    "            ph = len(y_true)\n",
    "            preds.extend(y_hat[j, :ph].tolist())\n",
    "            trues.extend(y_true.tolist())\n",
    "            times.extend(idx_block.tolist())\n",
    "\n",
    "    res = pd.DataFrame({\"datetime\": times, \"y_true\": trues, \"y_pred\": preds}).set_index(\"datetime\")\n",
    "    mse  = mean_squared_error(res[\"y_true\"], res[\"y_pred\"])\n",
    "    rmse = mean_squared_error(res[\"y_true\"], res[\"y_pred\"])\n",
    "    return res, mse, rmse\n",
    "\n",
    "# ---------- 4b) RECURSIVE 1-step × block_len (mirrors your TabPFN/linear recursion) ----------\n",
    "def flowstate_recursive(y: pd.Series, test_idx, starts, ctx_len, block_len, batch, device):\n",
    "    preds, trues, times = [], [], []\n",
    "    # work in batches of blocks\n",
    "    for i in range(0, len(starts), batch):\n",
    "        start_slice = [test_idx[s] for s in starts[i:i+batch] if s < len(test_idx)]\n",
    "        if not start_slice:\n",
    "            continue\n",
    "\n",
    "        # init contexts for these blocks\n",
    "        ctx, kept = build_batch_contexts_y(y, start_slice, ctx_len, device)\n",
    "        if ctx is None:\n",
    "            continue\n",
    "\n",
    "        # we will mutate a copy of the context for recursive stepping\n",
    "        ctx_work = ctx.clone()\n",
    "\n",
    "        # predict 1 step, append to context, repeat block_len times\n",
    "        batch_kept = ctx_work.shape[1]\n",
    "        step_preds = np.zeros((batch_kept, block_len), dtype=\"float32\")\n",
    "\n",
    "        for h in range(block_len):\n",
    "            with torch.inference_mode():\n",
    "                out = predictor(\n",
    "                    ctx_work, scale_factor=SCALE, prediction_length=1, batch_first=False\n",
    "                )\n",
    "            po = out.prediction_outputs  # (batch, num_quantiles, 1, 1)\n",
    "            if hasattr(out, \"quantile_values\"):\n",
    "                qs = torch.tensor(out.quantile_values, device=po.device)\n",
    "                q_idx = int(torch.argmin(torch.abs(qs - Q_MED)).item())\n",
    "            else:\n",
    "                q_idx = po.shape[1] // 2\n",
    "\n",
    "            y_hat_h = po[:, q_idx, 0, 0]              # (batch,)\n",
    "            step_preds[:, h] = y_hat_h.detach().cpu().numpy()\n",
    "\n",
    "            # shift: drop oldest, append current prediction\n",
    "            ctx_work = torch.cat([ctx_work[1:], y_hat_h.view(1, batch_kept, 1)], dim=0)\n",
    "\n",
    "        # stitch per block\n",
    "        for j, t0 in enumerate(kept):\n",
    "            start_row = int(np.where(test_idx == t0)[0][0])\n",
    "            idx_block = test_idx[start_row : start_row + block_len]\n",
    "            y_true = y.loc[idx_block].to_numpy(dtype=\"float32\")\n",
    "            ph = len(y_true)\n",
    "            preds.extend(step_preds[j, :ph].tolist())\n",
    "            trues.extend(y_true.tolist())\n",
    "            times.extend(idx_block.tolist())\n",
    "\n",
    "    res = pd.DataFrame({\"datetime\": times, \"y_true\": trues, \"y_pred\": preds}).set_index(\"datetime\")\n",
    "    mse  = mean_squared_error(res[\"y_true\"], res[\"y_pred\"])\n",
    "    rmse = mean_squared_error(res[\"y_true\"], res[\"y_pred\"])\n",
    "    return res, mse, rmse\n",
    "\n",
    "# ---------- 5) Run both modes ----------\n",
    "results_dir, mse_dir, rmse_dir = flowstate_direct(\n",
    "    df_canon[\"Y\"], test_idx, starts, CTX_LEN, block_len, BATCH, device\n",
    ")\n",
    "print(f\"[FlowState AR-only | DIRECT]  blocks={len(results_dir)//block_len}  MSE={mse_dir:.3f}  RMSE={rmse_dir:.3f}\")\n",
    "\n",
    "results_rec, mse_rec, rmse_rec = flowstate_recursive(\n",
    "    df_canon[\"Y\"], test_idx, starts, CTX_LEN, block_len, BATCH, device\n",
    ")\n",
    "print(f\"[FlowState AR-only | RECURSIVE]  blocks={len(results_rec)//block_len}  MSE={mse_rec:.3f}  RMSE={rmse_rec:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9727629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_64070/2136177935.py:12: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df_canon = df[[\"Y\",\"T\",\"RH\",\"AH\"]].sort_index().asfreq(freq)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ===== Chronos / Chronos-Bolt — AR-only, comparable to your TabPFN & linear =====\n",
    "# pip install -U chronos-forecasting\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from chronos import BaseChronosPipeline  # Chronos-T5 or Chronos-Bolt\n",
    "\n",
    "# ---------- 0) Canonical hourly data (apples-to-apples across models) ----------\n",
    "freq = \"H\"\n",
    "df_canon = df[[\"Y\",\"T\",\"RH\",\"AH\"]].sort_index().asfreq(freq)\n",
    "\n",
    "# causal fills (no future peek)\n",
    "df_canon[\"Y\"] = df_canon[\"Y\"].ffill()\n",
    "for c in [\"T\",\"RH\",\"AH\"]:\n",
    "    df_canon[c] = df_canon[c].ffill()\n",
    "\n",
    "# ---------- 1) Shared split & row-blocks ----------\n",
    "split_ts  = pd.Timestamp(\"2004-11-01 00:00:00\")\n",
    "block_len = 5  # set 3 if you want a 3-step horizon\n",
    "\n",
    "test_idx = df_canon.index[df_canon.index >= split_ts]\n",
    "starts   = np.arange(0, len(test_idx), block_len)  # 0,5,10,...\n",
    "\n",
    "# ---------- 2) Load a Chronos pipeline ----------\n",
    "# Bolt is fast & memory-efficient: \"amazon/chronos-bolt-small\" or \"amazon/chronos-bolt-base\"\n",
    "model_id   = \"amazon/chronos-bolt-small\"\n",
    "device_map = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype      = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "pipeline = BaseChronosPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=device_map,\n",
    "    torch_dtype=dtype,\n",
    ")\n",
    "\n",
    "# Keep Chronos context equal to your TabPFN lags for fairness\n",
    "CTX_LEN = 72\n",
    "BATCH   = 64  # drop to 16/32 if tight on memory\n",
    "\n",
    "# ---------- 3) Helper: build list of contexts (one per block start) ----------\n",
    "def build_context_list(y: pd.Series, start_ts_list, ctx_len):\n",
    "    yv, idx = y.to_numpy(dtype=\"float32\"), y.index\n",
    "    contexts, kept = [], []\n",
    "    for t0 in start_ts_list:\n",
    "        pos = idx.get_loc(t0)\n",
    "        if isinstance(pos, slice):  # guard against duplicate indices\n",
    "            pos = pos.start\n",
    "        if pos < ctx_len:\n",
    "            continue\n",
    "        contexts.append(torch.tensor(yv[pos-ctx_len:pos]))\n",
    "        kept.append(t0)\n",
    "    return contexts, kept\n",
    "\n",
    "# ---------- 4a) DIRECT: predict block_len in one call (Chronos-Bolt native) ----------\n",
    "def chronos_direct(y: pd.Series, test_idx, starts, ctx_len, block_len, batch):\n",
    "    preds, trues, times = [], [], []\n",
    "\n",
    "    for i in range(0, len(starts), batch):\n",
    "        start_slice = [test_idx[s] for s in starts[i:i+batch] if s < len(test_idx)]\n",
    "        if not start_slice:\n",
    "            continue\n",
    "\n",
    "        contexts, kept = build_context_list(y, start_slice, ctx_len)\n",
    "        if not contexts:\n",
    "            continue\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            # predict_quantiles returns (quantiles, mean)\n",
    "            # quantiles: [batch, pred_len, n_q]; mean: [batch, pred_len]\n",
    "            quantiles, mean = pipeline.predict_quantiles(\n",
    "                context=contexts,                # list of 1D tensors\n",
    "                prediction_length=block_len,\n",
    "                quantile_levels=[0.1, 0.5, 0.9],\n",
    "            )\n",
    "\n",
    "        y_hat = mean.numpy()  # (batch_kept, block_len) — use mean for MSE/RMSE\n",
    "\n",
    "        # stitch back to timestamps\n",
    "        for j, t0 in enumerate(kept):\n",
    "            start_row = int(np.where(test_idx == t0)[0][0])\n",
    "            idx_block = test_idx[start_row : start_row + block_len]\n",
    "            y_true = y.loc[idx_block].to_numpy(dtype=\"float32\")\n",
    "            ph = len(y_true)\n",
    "            preds.extend(y_hat[j, :ph].tolist())\n",
    "            trues.extend(y_true.tolist())\n",
    "            times.extend(idx_block.tolist())\n",
    "\n",
    "    results = pd.DataFrame({\"datetime\": times, \"y_true\": trues, \"y_pred\": preds}).set_index(\"datetime\")\n",
    "    mse  = mean_squared_error(results[\"y_true\"], results[\"y_pred\"])\n",
    "    rmse = mean_squared_error(results[\"y_true\"], results[\"y_pred\"])\n",
    "    return results, mse, rmse\n",
    "\n",
    "# ---------- 4b) RECURSIVE: 1-step × block_len (mirrors your TabPFN loop exactly) ----------\n",
    "def chronos_recursive(y: pd.Series, test_idx, starts, ctx_len, block_len, batch):\n",
    "    preds, trues, times = [], [], []\n",
    "\n",
    "    for i in range(0, len(starts), batch):\n",
    "        start_slice = [test_idx[s] for s in starts[i:i+batch] if s < len(test_idx)]\n",
    "        if not start_slice:\n",
    "            continue\n",
    "\n",
    "        contexts, kept = build_context_list(y, start_slice, ctx_len)\n",
    "        if not contexts:\n",
    "            continue\n",
    "\n",
    "        # mutable contexts (drop oldest, append prediction each step)\n",
    "        run_ctx = [c.clone().to(torch.float32) for c in contexts]  # one (ctx_len,) per block\n",
    "\n",
    "        step_preds = np.zeros((len(run_ctx), block_len), dtype=\"float32\")\n",
    "\n",
    "        for h in range(block_len):\n",
    "            with torch.inference_mode():\n",
    "                _, mean_h = pipeline.predict_quantiles(\n",
    "                    context=run_ctx, prediction_length=1, quantile_levels=[0.5]\n",
    "                )\n",
    "            y_hat_h = mean_h[:, 0].numpy()  # (batch,)\n",
    "\n",
    "            # update contexts (shift left + append)\n",
    "            for j in range(len(run_ctx)):\n",
    "                x = run_ctx[j].numpy()\n",
    "                x = np.concatenate([x[1:], y_hat_h[j:j+1]]).astype(\"float32\")\n",
    "                run_ctx[j] = torch.from_numpy(x)\n",
    "\n",
    "            step_preds[:, h] = y_hat_h\n",
    "\n",
    "        # stitch predictions to timestamps\n",
    "        for j, t0 in enumerate(kept):\n",
    "            start_row = int(np.where(test_idx == t0)[0][0])\n",
    "            idx_block = test_idx[start_row : start_row + block_len]\n",
    "            y_true = y.loc[idx_block].to_numpy(dtype=\"float32\")\n",
    "            ph = len(y_true)\n",
    "            preds.extend(step_preds[j, :ph].tolist())\n",
    "            trues.extend(y_true.tolist())\n",
    "            times.extend(idx_block.tolist())\n",
    "\n",
    "    results = pd.DataFrame({\"datetime\": times, \"y_true\": trues, \"y_pred\": preds}).set_index(\"datetime\")\n",
    "    mse  = mean_squared_error(results[\"y_true\"], results[\"y_pred\"])\n",
    "    rmse = mean_squared_error(results[\"y_true\"], results[\"y_pred\"])\n",
    "    return results, mse, rmse\n",
    "\n",
    "# ---------- 5) Run both modes ----------\n",
    "results_dir, mse_dir, rmse_dir = chronos_direct(df_canon[\"Y\"], test_idx, starts, CTX_LEN, block_len, BATCH)\n",
    "print(f\"[Chronos DIRECT]    blocks={len(results_dir)//block_len}  MSE={mse_dir:.3f}  RMSE={rmse_dir:.3f}\")\n",
    "\n",
    "results_rec, mse_rec, rmse_rec = chronos_recursive(df_canon[\"Y\"], test_idx, starts, CTX_LEN, block_len, BATCH)\n",
    "print(f\"[Chronos RECURSIVE] blocks={len(results_rec)//block_len}  MSE={mse_rec:.3f}  RMSE={rmse_rec:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034fe5b5",
   "metadata": {},
   "source": [
    "Yinglong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dd47ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages (4.50.3)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting xformers\n",
      "  Downloading xformers-0.0.32.post2.tar.gz (12.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Collecting torch>=2.8 (from xformers)\n",
      "  Using cached torch-2.8.0-cp311-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages (from torch>=2.8->xformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages (from torch>=2.8->xformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages (from torch>=2.8->xformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.8->xformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages (from jinja2->torch>=2.8->xformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages (from requests->transformers) (2025.4.26)\n",
      "Using cached transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "Using cached torch-2.8.0-cp311-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "Building wheels for collected packages: xformers\n",
      "  Building wheel for xformers (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for xformers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[213 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /private/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/pip-build-env-kzq5gl8a/overlay/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  \u001b[31m   \u001b[0m   cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "  \u001b[31m   \u001b[0m /private/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/pip-build-env-kzq5gl8a/overlay/lib/python3.11/site-packages/setuptools/dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         License :: OSI Approved :: BSD License\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   self._finalize_license_expression()\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m W1003 11:10:42.493000 48976 torch/utils/cpp_extension.py:615] Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_deprecation_warning.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/attn_bias_utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/checkpoint.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/test.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_cpp_lib.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/info.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/importing.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/vararg_kernel.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/flash_attn_3\n",
      "  \u001b[31m   \u001b[0m copying xformers/flash_attn_3/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/flash_attn_3\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/residual.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/input_projection.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_mem_eff_attention.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_indexing.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_revnet.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_swiglu.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_merge_attentions.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_tiled_matmul.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_nystrom_utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_attn_decoding.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_sequence_parallel_fused.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_sddmm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_sp24.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_core.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/rmsnorm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/modpar_layers.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/swiglu_op.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/unbind.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/rope_padded.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/seqpar.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/sequence_parallel_fused_ops.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/sp24.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/tree_attention.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/common.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/differentiable_collectives.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/tiled_matmul.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/indexing.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/device_limits.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/find_slowest.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/profiler_dcgm_impl.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/profiler_dcgm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/api.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/profile_analyzer.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/profiler.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/_csr_ops.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/blocksparse_tensor.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/csr_tensor.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/fused_softmax.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_blocksparse_attn_interface.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_blocksparse_attention.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/bert_padding.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_og.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_interface.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/_sputnik_sparse.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/core.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/fourier_mix.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/scaled_dot_product.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/attention_mask.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/attention_patterns.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/sparsity_config.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/base.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/batch_submit.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/batch_fetch_results.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/run_with_submitit.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/run_tasks.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/run_grid_search.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA/code\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/code/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA/code\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/code/model_wrapper.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA/code\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/code/dataset.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA/code\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/_triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/_triton/k_scaled_index_add.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/_triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/_triton/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/_triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/_triton/matmul_perf_model.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/_triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/_triton/rope_padded_kernels.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/_triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/_triton/tiled_matmul_kernels.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/_triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/_triton/k_index_select_cat.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/_triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/_triton/rmsnorm_kernels.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/_triton\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/flash3.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/dispatch.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/attn_bias.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/ck.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/common.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/torch_attention_compat.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/flash.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/cutlass.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/merge_training.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/ck_splitk.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/triton_splitk.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha/_triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/_triton/splitk_kernels.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha/_triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/_triton/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha/_triton\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/losses/cross_entropy.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/losses/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/layers/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/layers/patch_embed.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/layers/rotary.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_amd/bwd_prefill_split.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_amd/fwd_ref.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_amd/fp8.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_amd/fwd_prefill.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_amd/interface_fa.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_amd/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_amd/bwd_prefill_onekernel.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_amd/test.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_amd/bench.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_amd/bwd_prefill.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_amd/utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_amd/train.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_amd/fwd_decode.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_amd/bwd_ref.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_amd/bwd_prefill_fused.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/pretrained.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/generation.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/library.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/benchmark.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/distributed.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/testing.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/torch.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/bigcode.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/gptj.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/opt.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/llama.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/vit.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/btlm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/baichuan.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/bert.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/falcon.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/gpt_neox.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/gpt.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/activations.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/fused_dense.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/rms_norm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/layer_norm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/embedding.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/mlp.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/block.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/mha.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/cross_entropy.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/linear.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/k_activations.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/mlp.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/rotary.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/layer_norm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'xformers._C' extension\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/autograd\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/cpu\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/sparse24\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/swiglu\n",
      "  \u001b[31m   \u001b[0m clang++ -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/include -arch arm64 -fPIC -O2 -isystem /Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/include -arch arm64 -I/private/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/pip-install-e9ccum6u/xformers_a488dba8f26f4d018a4caacfe0b7c4d8/xformers/csrc -I/private/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/pip-build-env-kzq5gl8a/overlay/lib/python3.11/site-packages/torch/include -I/private/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/pip-build-env-kzq5gl8a/overlay/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/Users/kilianrunnwerth/Desktop/Python/Kaggel/.conda/include/python3.11 -c xformers/csrc/attention/attention.cpp -o build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/attention.o -O3 -std=c++17 -DPy_LIMITED_API=0x03090000 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPy_LIMITED_API=0x03090000 -DTORCH_EXTENSION_NAME=_C\n",
      "  \u001b[31m   \u001b[0m clang++: error: unsupported option '-fopenmp'\n",
      "  \u001b[31m   \u001b[0m error: command '/usr/bin/clang++' failed with exit code 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[31m  ERROR: Failed building wheel for xformers\u001b[0m\u001b[31m\n",
      "\u001b[0mFailed to build xformers\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mfailed-wheel-build-for-install\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Failed to build installable wheels for some pyproject.toml based projects\n",
      "\u001b[31m╰─>\u001b[0m xformers\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers xformers  # xformers optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7488f82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q9/d545ygdx5rj6lg_00gpykppw0000gn/T/ipykernel_48738/768323795.py:13: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df_canon = df[[\"Y\",\"T\",\"RH\",\"AH\"]].sort_index().asfreq(freq)\n",
      "A new version of the following files was downloaded from https://huggingface.co/qcw2333/YingLong_6m:\n",
      "- model_config.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Encountered exception while importing flash_attn: No module named 'flash_attn'\n",
      "Encountered exception while importing rotary_emb: No module named 'rotary_emb'\n",
      "Encountered exception while importing dropout_layer_norm: No module named 'dropout_layer_norm'\n",
      "Encountered exception while importing xformers: No module named 'xformers'\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "This modeling file requires the following packages that were not found in your environment: flash_attn, rotary_emb, dropout_layer_norm, xformers. Run `pip install flash_attn rotary_emb dropout_layer_norm xformers`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m dtype    = torch.bfloat16 \u001b[38;5;28;01mif\u001b[39;00m (device == \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m torch.float32\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# HF quickstart uses: AutoModelForCausalLM.from_pretrained(..., trust_remote_code=True).generate(future_token=...)  :contentReference[oaicite:3]{index=3}\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m)\u001b[49m.to(device)\n\u001b[32m     34\u001b[39m model.eval()\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Keep context equal to your TabPFN lags for fairness (you can raise later)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:562\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    560\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[32m    561\u001b[39m     class_ref = config.auto_map[\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m     model_class = \u001b[43mget_class_from_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    565\u001b[39m     _ = hub_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    566\u001b[39m     \u001b[38;5;28mcls\u001b[39m.register(config.\u001b[34m__class__\u001b[39m, model_class, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages/transformers/dynamic_module_utils.py:541\u001b[39m, in \u001b[36mget_class_from_dynamic_module\u001b[39m\u001b[34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[39m\n\u001b[32m    539\u001b[39m     code_revision = revision\n\u001b[32m    540\u001b[39m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m final_module = \u001b[43mget_cached_module_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodule_file\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.py\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m get_class_in_module(class_name, final_module, force_reload=force_download)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages/transformers/dynamic_module_utils.py:366\u001b[39m, in \u001b[36mget_cached_module_file\u001b[39m\u001b[34m(pretrained_model_name_or_path, module_file, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    365\u001b[39m \u001b[38;5;66;03m# Check we have all the requirements in our environment\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m modules_needed = \u001b[43mcheck_imports\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_module_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[38;5;66;03m# Now we move the module inside our cached dynamic modules.\u001b[39;00m\n\u001b[32m    369\u001b[39m full_submodule = TRANSFORMERS_DYNAMIC_MODULE_NAME + os.path.sep + submodule\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Python/Kaggel/.conda/lib/python3.11/site-packages/transformers/dynamic_module_utils.py:198\u001b[39m, in \u001b[36mcheck_imports\u001b[39m\u001b[34m(filename)\u001b[39m\n\u001b[32m    195\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_packages) > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    199\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis modeling file requires the following packages that were not found in your environment: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    200\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(missing_packages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Run `pip install \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(missing_packages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    201\u001b[39m     )\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m get_relative_imports(filename)\n",
      "\u001b[31mImportError\u001b[39m: This modeling file requires the following packages that were not found in your environment: flash_attn, rotary_emb, dropout_layer_norm, xformers. Run `pip install flash_attn rotary_emb dropout_layer_norm xformers`"
     ]
    }
   ],
   "source": [
    "# ===================== YingLong — AR-only, TabPFN-comparable =====================\n",
    "# \n",
    "# If you hit flash-attn import issues, use CPU or the 6m model (see note below).  :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# ---------- 0) Canonical hourly data (same as the other models) ----------\n",
    "freq = \"H\"\n",
    "df_canon = df[[\"Y\",\"T\",\"RH\",\"AH\"]].sort_index().asfreq(freq)\n",
    "df_canon[\"Y\"] = df_canon[\"Y\"].ffill()\n",
    "for c in [\"T\",\"RH\",\"AH\"]:\n",
    "    df_canon[c] = df_canon[c].ffill()\n",
    "\n",
    "# ---------- 1) Shared split & row-blocks ----------\n",
    "split_ts  = pd.Timestamp(\"2004-11-01 00:00:00\")\n",
    "block_len = 5  # set 3 if you want a 3-step horizon\n",
    "test_idx  = df_canon.index[df_canon.index >= split_ts]\n",
    "starts    = np.arange(0, len(test_idx), block_len)  # 0,5,10,...\n",
    "\n",
    "# ---------- 2) Load YingLong ----------\n",
    "# Pick a size. Start with 6m for reliability; switch to 110m/300m if you have GPU headroom.\n",
    "model_id = \"qcw2333/YingLong_6m\"  # alternatives: \"qcw2333/YingLong_110m\", \"qcw2333/YingLong_300m\"  :contentReference[oaicite:2]{index=2}\n",
    "device   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype    = torch.bfloat16 if (device == \"cuda\") else torch.float32\n",
    "\n",
    "# HF quickstart uses: AutoModelForCausalLM.from_pretrained(..., trust_remote_code=True).generate(future_token=...)  :contentReference[oaicite:3]{index=3}\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, trust_remote_code=True, torch_dtype=dtype\n",
    ").to(device)\n",
    "model.eval()\n",
    "\n",
    "# Keep context equal to your TabPFN lags for fairness (you can raise later)\n",
    "CTX_LEN = 72\n",
    "BATCH   = 64  # drop to 16/32 if memory is tight\n",
    "\n",
    "# ---------- 3) Helpers ----------\n",
    "def build_batch_contexts_y(y: pd.Series, start_ts_list, ctx_len):\n",
    "    \"\"\"\n",
    "    Return (X, kept):\n",
    "      X: FloatTensor [batch, ctx_len] of past Y strictly before each block start\n",
    "      kept: the subset of timestamps that had >= ctx_len history\n",
    "    \"\"\"\n",
    "    yv, idx = y.to_numpy(dtype=\"float32\"), y.index\n",
    "    rows, kept = [], []\n",
    "    for t0 in start_ts_list:\n",
    "        pos = idx.get_loc(t0)\n",
    "        if isinstance(pos, slice): pos = pos.start\n",
    "        if pos < ctx_len:  # not enough history\n",
    "            continue\n",
    "        rows.append(yv[pos-ctx_len:pos])\n",
    "        kept.append(t0)\n",
    "    if not rows:\n",
    "        return None, []\n",
    "    X = torch.tensor(np.stack(rows, axis=0), dtype=torch.float32, device=device)\n",
    "    # cast up to model dtype on GPU\n",
    "    return (X.to(dtype) if device == \"cuda\" else X), kept\n",
    "\n",
    "@torch.inference_mode()\n",
    "def yinglong_direct(y: pd.Series, test_idx, starts, ctx_len, block_len, batch):\n",
    "    preds, trues, times = [], [], []\n",
    "    for i in range(0, len(starts), batch):\n",
    "        start_slice = [test_idx[s] for s in starts[i:i+batch] if s < len(test_idx)]\n",
    "        if not start_slice: continue\n",
    "        X, kept = build_batch_contexts_y(y, start_slice, ctx_len)\n",
    "        if X is None: continue\n",
    "\n",
    "        # GENERATE block_len steps in one shot (DIRECT)\n",
    "        out = model.generate(X, future_token=block_len)  # -> [batch, block_len]  :contentReference[oaicite:4]{index=4}\n",
    "        y_hat = out.detach().to(torch.float32).cpu().numpy()\n",
    "\n",
    "        for j, t0 in enumerate(kept):\n",
    "            start_row = int(np.where(test_idx == t0)[0][0])\n",
    "            idx_block = test_idx[start_row : start_row + block_len]\n",
    "            y_true = y.loc[idx_block].to_numpy(dtype=\"float32\")\n",
    "            ph = len(y_true)\n",
    "            preds.extend(y_hat[j, :ph].tolist())\n",
    "            trues.extend(y_true.tolist())\n",
    "            times.extend(idx_block.tolist())\n",
    "\n",
    "    res  = pd.DataFrame({\"datetime\": times, \"y_true\": trues, \"y_pred\": preds}).set_index(\"datetime\")\n",
    "    mse  = mean_squared_error(res[\"y_true\"], res[\"y_pred\"])\n",
    "    rmse = mean_squared_error(res[\"y_true\"], res[\"y_pred\"], squared=False)\n",
    "    return res, mse, rmse\n",
    "\n",
    "@torch.inference_mode()\n",
    "def yinglong_recursive(y: pd.Series, test_idx, starts, ctx_len, block_len, batch):\n",
    "    preds, trues, times = [], [], []\n",
    "    for i in range(0, len(starts), batch):\n",
    "        start_slice = [test_idx[s] for s in starts[i:i+batch] if s < len(test_idx)]\n",
    "        if not start_slice: continue\n",
    "        X, kept = build_batch_contexts_y(y, start_slice, ctx_len)\n",
    "        if X is None: continue\n",
    "\n",
    "        # RECURSIVE: 1-step at a time, shift window, append prediction\n",
    "        B = X.shape[0]\n",
    "        step_preds = np.zeros((B, block_len), dtype=\"float32\")\n",
    "        ctx_work = X.clone()\n",
    "\n",
    "        for h in range(block_len):\n",
    "            out = model.generate(ctx_work, future_token=1)        # [B, 1]\n",
    "            y_hat_h = out.squeeze(-1) if out.ndim == 3 else out   # be tolerant of shape\n",
    "            y_hat_h = y_hat_h.view(B, -1)[:, 0]                   # [B]\n",
    "            step_preds[:, h] = y_hat_h.detach().to(torch.float32).cpu().numpy()\n",
    "            # update rolling context (drop oldest, append prediction)\n",
    "            ctx_work = torch.cat([ctx_work[:, 1:], y_hat_h[:, None].to(ctx_work.dtype)], dim=1)\n",
    "\n",
    "        for j, t0 in enumerate(kept):\n",
    "            start_row = int(np.where(test_idx == t0)[0][0])\n",
    "            idx_block = test_idx[start_row : start_row + block_len]\n",
    "            y_true = y.loc[idx_block].to_numpy(dtype=\"float32\")\n",
    "            ph = len(y_true)\n",
    "            preds.extend(step_preds[j, :ph].tolist())\n",
    "            trues.extend(y_true.tolist())\n",
    "            times.extend(idx_block.tolist())\n",
    "\n",
    "    res  = pd.DataFrame({\"datetime\": times, \"y_true\": trues, \"y_pred\": preds}).set_index(\"datetime\")\n",
    "    mse  = mean_squared_error(res[\"y_true\"], res[\"y_pred\"])\n",
    "    rmse = mean_squared_error(res[\"y_true\"], res[\"y_pred\"], squared=False)\n",
    "    return res, mse, rmse\n",
    "\n",
    "# ---------- 4) Run both modes ----------\n",
    "results_dir, mse_dir, rmse_dir = yinglong_direct(df_canon[\"Y\"], test_idx, starts, CTX_LEN, block_len, BATCH)\n",
    "print(f\"[YingLong DIRECT]    blocks={len(results_dir)//block_len}  MSE={mse_dir:.3f}  RMSE={rmse_dir:.3f}\")\n",
    "\n",
    "results_rec, mse_rec, rmse_rec = yinglong_recursive(df_canon[\"Y\"], test_idx, starts, CTX_LEN, block_len, BATCH)\n",
    "print(f\"[YingLong RECURSIVE] blocks={len(results_rec)//block_len}  MSE={mse_rec:.3f}  RMSE={rmse_rec:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39864482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:p-8346:t-8754634944:modeling_flowstate.py:__init__:Number of encoder parameters: 7885.8240000000005k\n",
      "INFO:p-8346:t-8754634944:modeling_flowstate.py:__init__:Number of dencoder parameters: 1181.952k (14.99%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 9, 960, 1])\n"
     ]
    }
   ],
   "source": [
    "from tsfm_public import FlowStateForPrediction\n",
    "import torch\n",
    "ädevice= 'cuda'\n",
    "predictor = FlowStateForPrediction.from_pretrained(\"ibm-granite/granite-timeseries-flowstate-r1\")\n",
    "time_series = torch.randn((2048, 32, 1)) # context, batch, n_ch\n",
    "forecast = predictor(time_series, scale_factor=0.25, prediction_length=960, batch_first=False)\n",
    "print(forecast.prediction_outputs.shape) # torch.Size([32, 9, 48, 1]) (batch, quantiles, forecast_length, n_ch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
